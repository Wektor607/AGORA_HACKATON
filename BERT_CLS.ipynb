{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\setuptools\\distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = 'cointegrated/rubert-tiny'\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "model_path = 'cointegrated/rubert-tiny'\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_features = model.bert.encoder.layer[1].output.dense.out_features\n",
    "out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    train_path = 'train.csv'\n",
    "    test_path = 'test.csv'\n",
    "    lr = 1e-5\n",
    "    max_len = 512\n",
    "    train_bs = 64\n",
    "    valid_bs = 32\n",
    "    train_pcent = 0.99\n",
    "    num_workers = 8\n",
    "    bert_model = model#'bert-base-uncased'\n",
    "    tokenizer = tokenizer#transformers.BertTokenizer.from_pretrained(bert_model, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgroData(Dataset):\n",
    "    def __init__(self, review, target):\n",
    "        self.review = review\n",
    "        self.encoder = {i: k for k, i in enumerate(np.unique(target))}\n",
    "        self.target = [self.encoder[j] for j in target]\n",
    "        self.tokenizer = Config.tokenizer\n",
    "        self.max_len = Config.max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.review)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.review[idx])\n",
    "        target = self.target[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = pd.read_csv('train.csv')\n",
    "val_ds = pd.read_csv('val.csv')\n",
    "test_ds = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = AgroData(train_ds['merged'], train_ds['reference_id'])\n",
    "train_loader = DataLoader(ds, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = AgroData(train_ds['merged'], train_ds['reference_id'])\n",
    "ds_val = AgroData(val_ds['merged'], val_ds['reference_id'])\n",
    "ds_test = AgroData(test_ds['merged'], test_ds['reference_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=2, shuffle=True)\n",
    "dl_val = DataLoader(ds_val, batch_size=2, shuffle=False)\n",
    "dl_test = DataLoader(ds_test, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(pl.LightningModule):\n",
    "    def __init__(self) -> None:\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.bert = model\n",
    "        self.bert.classifier = nn.Linear(312, 471)\n",
    "        self.all_targets = []\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, ids, mask) -> torch.Tensor:\n",
    "        output = self.bert(ids, attention_mask=mask)\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        ids = batch['input_ids']\n",
    "        mask = batch['attention_mask']\n",
    "        targets = batch['targets']\n",
    "\n",
    "        outputs = self(ids=ids, mask=mask)\n",
    "\n",
    "        train_loss = self.loss_fn(outputs.logits, targets)\n",
    "        return {'loss': train_loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ids = batch['input_ids']\n",
    "        mask = batch['attention_mask']\n",
    "        targets = batch['targets']\n",
    "\n",
    "        outputs = self(ids=ids, mask=mask)\n",
    "        \n",
    "        valid_loss = self.loss_fn(outputs.logits, targets)\n",
    "        return {'val_loss': valid_loss}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = transformers.AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "        return optimizer\n",
    "    \n",
    "    def predict(self, text):\n",
    "        encoding = Config.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        out = {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "        \n",
    "        input_ids = out[\"input_ids\"].to(self.device)\n",
    "        attention_mask = out[\"attention_mask\"].to(self.device)\n",
    "        \n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids.unsqueeze(0),\n",
    "            attention_mask=attention_mask.unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        prediction = torch.argmax(outputs.logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "modelN = BERTModel()\n",
    "trainer = pl.Trainer(max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type                          | Params\n",
      "----------------------------------------------------------\n",
      "0 | bert    | BertForSequenceClassification | 11.9 M\n",
      "1 | loss_fn | CrossEntropyLoss              | 0     \n",
      "----------------------------------------------------------\n",
      "11.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.9 M    Total params\n",
      "47.726    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1114/1114 [17:29<00:00,  1.06it/s, loss=6, v_num=19]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(modelN, dl_train, dl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text):\n",
    "        encoding = Config.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=Config.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        out = {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "        \n",
    "        input_ids = out[\"input_ids\"].to(model.device)\n",
    "        attention_mask = out[\"attention_mask\"].to(model.device)\n",
    "        \n",
    "        outputs = model.bert(\n",
    "            input_ids=input_ids.unsqueeze(0),\n",
    "            attention_mask=attention_mask.unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        prediction = torch.argmax(outputs.logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270bcbb525544c87aa8b624200235912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/554 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.14801444043322%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "all = len(dl_test)\n",
    "\n",
    "for el in tqdm(range(test_ds.shape[0])):\n",
    "    correct = ds_test.encoder[test_ds['reference_id'][el]]\n",
    "    prediction = predict(modelN, test_ds['merged'][el])\n",
    "\n",
    "    correct += (prediction==correct)\n",
    "\n",
    "print(f\"Accuracy: {correct/all * 100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelN.state_dict(), \"bert1_0.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
